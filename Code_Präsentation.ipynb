{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.input_size = n_input\n",
    "        self.output_size = n_output\n",
    "        self.layers = []\n",
    "        self.current_last_n = self.input_size\n",
    "        self.layer_outputs = []\n",
    "        np.random.seed(1)\n",
    "\n",
    "    def add_layer(self, n_neurons, activation=\"sigmoid\"):\n",
    "        # One more neuron than last layer because of bias\n",
    "        weights = np.random.uniform(low=-0.3, high=0.3, size=(self.current_last_n+1, n_neurons))\n",
    "        self.current_last_n = n_neurons\n",
    "        \n",
    "        # building layers\n",
    "        hidden_layer = dict()\n",
    "        hidden_layer[\"weights\"] = weights\n",
    "        hidden_layer[\"activation\"] = activation\n",
    "        self.layers.append(hidden_layer)\n",
    "\n",
    "    def compile(self):\n",
    "        # connect output layer with last hidden layer\n",
    "        self.add_layer(self.output_size, activation=\"sigmoid\")\n",
    "\n",
    "    def predict(self, input):\n",
    "        \n",
    "        # add column of ones because of bias neuron\n",
    "        input = np.array(input, dtype=float)\n",
    "        r, c = input.shape\n",
    "        input = np.c_[input, np.ones(r)]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = np.dot(input, layer[\"weights\"])\n",
    "            output = self.activation(output, layer[\"activation\"])\n",
    "            \n",
    "            # safe outputs of all layers for back propagation\n",
    "            self.layer_outputs.append(output)\n",
    "\n",
    "            # add column of ones because of bias neuron\n",
    "            if layer != self.layers[-1]:\n",
    "                r, c = output.shape\n",
    "                output = np.c_[output, np.ones(r)]\n",
    "\n",
    "            input = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def train(self, input, labels, epochs, learning_rate):\n",
    "        input = np.array(input, dtype=float)\n",
    "        labels = np.array(labels, dtype=float)\n",
    "\n",
    "        for epoch in range(epochs+1):\n",
    "            output = self.predict(input)\n",
    "            delta = []\n",
    "            \n",
    "            # calculation for output layer is slightly different\n",
    "            errors = np.array(self.layer_outputs[-1] - labels)\n",
    "            derivatives = self.derivative(self.layer_outputs[-1], self.layers[-1][\"activation\"])\n",
    "            delta.append(errors * derivatives)\n",
    "            \n",
    "            # go through all layers from back to front and calculate \"deltas\"\n",
    "            for i in range(1, len(self.layer_outputs)):\n",
    "                errors = np.dot(delta[-1], self.layers[-i][\"weights\"].T[:,:-1])\n",
    "                derivatives = self.derivative(self.layer_outputs[-i-1], self.layers[-i][\"activation\"])\n",
    "                delta.append(errors * derivatives)\n",
    "            \n",
    "            # update weights\n",
    "            self.update_weights(input, delta, learning_rate)\n",
    "\n",
    "            if epoch % 200 is 0:\n",
    "                #print(output[:3])\n",
    "                #print(labels[:3])\n",
    "                #print(np.sum(output[0]))\n",
    "                sum_error = np.sum((labels - output) ** 2)\n",
    "                print(f\"epoch: {epoch}, error: {sum_error}\")\n",
    "\n",
    "\n",
    "\n",
    "    def update_weights(self, input, delta, learning_rate):\n",
    "\n",
    "        # add column of ones to input for bias neurons\n",
    "        i = -1\n",
    "        r, c = input.shape\n",
    "        input = np.c_[input, np.ones(r)]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            delta_values = delta.pop()\n",
    "\n",
    "            dot_product = np.dot(input.T, delta_values)\n",
    "            #print(delta_values.shape)\n",
    "            #print(dot_product.shape)\n",
    "            #print(layer[\"weights\"].shape)\n",
    "            layer[\"weights\"] -= learning_rate * dot_product\n",
    "\n",
    "            #if layer != self.layers[-1]:\n",
    "            #layer[\"weights\"][-1,:] -= learning_rate * delta_values[:, :-1].T\n",
    "            i += 1\n",
    "            input = self.layer_outputs[i]\n",
    "\n",
    "            # add column of ones to input for bias neurons\n",
    "            r, c = input.shape\n",
    "            input = np.c_[input, np.ones(r)]\n",
    "\n",
    "        self.layer_outputs = []\n",
    "\n",
    "    def evaluate(self, inputs, labels):\n",
    "        outputs = self.predict(inputs)\n",
    "        outputs = (outputs == outputs.max(axis=1)[:,None])\n",
    "        correct = np.sum((labels == outputs).all(1))\n",
    "        total = len(inputs)\n",
    "\n",
    "        print(\"\\nCorrectly classified: \", correct)\n",
    "        print(\"Total eval data: \", total)\n",
    "        print(\"Accuracy: \", correct/total)\n",
    "\n",
    "    def activation(self, inputs, activation):\n",
    "        if activation is \"sigmoid\":\n",
    "            sigmoid_v = np.vectorize(self.sigmoid)\n",
    "            return sigmoid_v(inputs)\n",
    "        elif activation is \"relu\":\n",
    "            relu_v = np.vectorize(self.relu)\n",
    "            return relu_v(inputs)\n",
    "        elif activation is \"softmax\":\n",
    "            return self.softmax(inputs)\n",
    "\n",
    "    def derivative(self, inputs, activation):\n",
    "        if activation is \"sigmoid\":\n",
    "            sigmoid_derivative_v = np.vectorize(self.sigmoid_derivative)\n",
    "            return sigmoid_derivative_v(inputs)\n",
    "        elif activation is \"relu\":\n",
    "            relu_derivative_v = np.vectorize(self.relu_derivative)\n",
    "            return relu_derivative_v(inputs)\n",
    "        elif activation is \"softmax\":\n",
    "            return self.softmax_derivative(inputs)\n",
    "\n",
    "    def error(self, actual, expected):\n",
    "        return\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        x = 10 if x > 10 else x\n",
    "        x = -10 if x < -10 else x\n",
    "        sig = 1 / (1 + math.exp(-x))\n",
    "        return sig\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return max(0.0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0) * 1\n",
    "\n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    def softmax_derivative(self, x):\n",
    "        I = np.eye(x.shape[1], x.shape[0])\n",
    "\n",
    "        return self.softmax(x) * (I - self.softmax(x).T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 185, 159, 151, 60, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 222, 254, 254, 254, 254, 241, 198, 198, 198, 198, 198, 198, 198, 198, 170, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 67, 114, 72, 114, 163, 227, 254, 225, 254, 254, 254, 250, 229, 254, 254, 140, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 66, 14, 67, 67, 67, 59, 21, 236, 254, 106, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 253, 209, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 233, 255, 83, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 129, 254, 238, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 249, 254, 62, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 133, 254, 187, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 205, 248, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 126, 254, 182, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 75, 251, 240, 57, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 221, 254, 166, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 203, 254, 219, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38, 254, 254, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 224, 254, 115, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 133, 254, 254, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 61, 242, 254, 254, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 121, 254, 254, 219, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 121, 254, 207, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Label: 7\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "# Importing MNIST dataset\n",
    "with open('mnist.csv') as csvfile:\n",
    "    data = csv.reader(csvfile)\n",
    "    next(data, None)\n",
    "    for row in data:\n",
    "        labels.append(row.pop(0))\n",
    "        inputs.append(list(map(int, row)))\n",
    "    \n",
    "    print(inputs[0])\n",
    "    print(\"Label:\", labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\.conda\\envs\\bifml2\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Encoding Labels\n",
    "input_size = len(inputs[0])\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded_labels = encoder.fit_transform(np.array(labels).reshape(-1,1))\n",
    "onehot_encoded_labels = np.array(onehot_encoded_labels, dtype=float)\n",
    "\n",
    "# Scaling Input Data\n",
    "inputs = np.array(inputs, dtype=float)\n",
    "inputs = inputs / 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Neural Network\n",
    "\n",
    "network = neural_network(input_size, 10)\n",
    "network.add_layer(8, activation=\"sigmoid\")\n",
    "network.add_layer(4, activation=\"sigmoid\")\n",
    "network.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\.conda\\envs\\bifml2\\lib\\site-packages\\ipykernel_launcher.py:41: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, error: 9894.570631834442\n",
      "epoch: 200, error: 3598.6364652866323\n",
      "epoch: 400, error: 3598.5688168361858\n",
      "epoch: 600, error: 3598.5147529965966\n",
      "epoch: 800, error: 3598.3339947913837\n",
      "epoch: 1000, error: 3648.951881344056\n",
      "epoch: 1200, error: 3595.0742700451856\n",
      "epoch: 1400, error: 3132.9005533752133\n",
      "epoch: 1600, error: 2257.2537982981607\n",
      "epoch: 1800, error: 1667.4983648933248\n",
      "epoch: 2000, error: 1482.7011602746675\n",
      "epoch: 2200, error: 1169.2182529325933\n",
      "epoch: 2400, error: 1263.6317202546427\n",
      "epoch: 2600, error: 979.8990791686776\n",
      "epoch: 2800, error: 914.1105381596312\n",
      "epoch: 3000, error: 745.1766351183812\n",
      "epoch: 3200, error: 754.1551043267319\n",
      "epoch: 3400, error: 929.3961800990985\n",
      "epoch: 3600, error: 697.7994774457461\n",
      "epoch: 3800, error: 755.3743495307857\n",
      "epoch: 4000, error: 361.11963596215946\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "np.random.seed(1)\n",
    "network.train(inputs[:4000], onehot_encoded_labels[:4000], 4000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correctly classified:  1625\n",
      "Total eval data:  2000\n",
      "Accuracy:  0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\.conda\\envs\\bifml2\\lib\\site-packages\\ipykernel_launcher.py:41: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "network.evaluate(inputs[8000:], onehot_encoded_labels[8000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
